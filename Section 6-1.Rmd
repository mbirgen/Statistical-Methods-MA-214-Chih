---
title: "t-distribution"
author: "Dr. Birgen"
output: html_notebook
---
# Section 6.1 The \(t\)-Variable and Sampling Distribution of Numerical Variables (N1) 
In this section, we transition away from categorical variables and begin looking at the sampling distributions of numerical variables.

### Remark 6.1.1. 
Recall from Section 1.4 that given a numerical variable \(X\) and a sample from \(X\) of size \(n\) ,we can obtain a sample mean \(\bar{x}\).

As we know, each time we sample \(X\), we obtain potentially a different \(\bar{x}\), so \(\bar{x}\) is a random variable from a sampling distribution. The random variable \(\bar{x}\) will play the role similar to that of \(\hat{p}\) in Chapter 5. We recognize \(\bar{x}\) as a point estimate for the true mean of \(X\text{:}\) \(\mu_X\).

### Exploration 6.1.1. Rolling Dice and Averaging the results.
Suppose I roll `dice_num` dice and take the average result. Let \(X\) be the random variable indicating the outcome of a six-sided die.
(a) Following Section 2.4,find \(E(X), Var(X), \sigma_X\).
```{r}

```

(b) Let \(A_2\) be the average of two die rolls: \(A_2=\frac{1}{2}(X_1+X_2).\) Using Remark 2.5.1, find \(E(A_2), Var(A_2), \sigma_{A_2}.\)
```{r}

```

(c) Run the following code to simulate trials=1000 trials where one rolls num_dice=2 dice and takes the average, and display the mean and standard deviation of these outcomes: 
```{r}
trials=1000
num_dice=2

averageroll=rep(NA, trials)

for(i in 1:trials){
  averageroll[i]=sum(sample(c(1,2,3,4,5,6), num_dice, replace=TRUE))/num_dice
}
```

How do these values compare to what you found in (b)?
(d) Let \(A_3\) be the average of two die rolls: \(A_2=\frac{1}{3}(X_1+X_2+X_3).\) Using Remark 2.5.1, find \(E(A_3), Var(A_3), \sigma_{A_3}.\)
```{r}

```

(e) Re-run the code in (c) with `num_dice=3`, how do those values compare to what you found in (d)?

>

(f) Re-run the code in (c) with `num_dice=30`, `num_dice=50`, `num_dice=100` What do we notice about the mean and sd as num_dice increases? How does the shape of the graph change?

>

## Subsection 6.1.1 \(t\)-Variables 
As we saw in Exploration 6.1.1, we saw a phenomena very similar to what we saw in Theorem 4.1.4. In Theorem 4.1.4, the sampling distribution for \(\hat{p}\) was approximately normal. We have a similar result for the sampling distribution for \(\bar{x}\).

### Theorem 6.1.2. The Central Limit Theorem (Numerical). 

Let \(X\) be a numerical variable with true mean \(\mu\) and standard deviation \(\sigma\).Then for a sufficiently large sample (typically \(\geq 30\)) the sampling distribution of \(\bar{m}\) is approximately normally distributed with parameters
\begin{equation*} \mu_{\bar{x}}=\mu,\ \ \ SE_{\bar{m}}=\frac{\sigma}{\sqrt{n}}. \end{equation*} 

### Remark 6.1.3. 
We can see how the average die rolls of Exploration 6.1.1 follow this. Run the following code to plot the histogram from Exploration 6.1.1, along with a normal curve with parameters mean `mu=3.5` and standard deviation `SE=sqrt(35/12)/sqrt(num_dice)`: 
```{r}
trials=1000
num_dice=30

mu=3.5
SE=sqrt(35/12)/sqrt(num_dice)

averageroll=rep(NA, trials)

for(i in 1:trials){
  averageroll[i]=sum(sample(c(1,2,3,4,5,6), num_dice, replace=TRUE))/num_dice
}
hist(averageroll, breaks=25, prob=TRUE)
curve(dnorm(x, mean=mu, sd=SE), col="darkblue", lwd=2, add=TRUE, yaxt="n")
print(mean(averageroll))
print(sd(averageroll))
```

Edit `num_dice` and see how the curve matches the histogram.

### Activity 6.1.2. Variation in Parameters. 

With categorical variables, the mean and standard deviation of the sampling distribution is totally determined by one parameter: \(p\). We can see this in the statement of Theorem 4.1.4. So when we assume a null hypothesis and set \(p=p_0\) ,this strictly determines the hypothetical sampling distribution.

As we see in Theorem 6.1.2, the sampling distribution for \(\bar{x}\) is determined by both \(\mu\) and \(\sigma\).So if one were to conduct a hypothesis test and set \(\mu=\mu_0\) ,there's still the question of what \(\sigma\) is. The general practice is to assume it's the same as the sample standard deviation, but how good of an estimate is this?

Run the following code to download and the `ames.csv` data set which contains information of houses in Ames, Iowa, and to see it's variable names: 
```{r}
ames = read.csv("https://github.com/TienChih/tbil-stats/raw/main/data/ames.csv")

names(ames)
```

Click here to learn more about this data set: https://www.openintro.org/data/index.php?data=ames.

(a) Run the following code to sample 30 random houses from Ames, Iowa and display their housing prices: 
```{r}
n=30
index = sample(1:nrow(ames), n)
samp=ames[index,]
samp$price
```

(b) Run the following code to show the standard deviation of housing prices in this sample: 
```{r}
sd(samp$price)
```

(c) Run the following code to sample 30 new random houses from Ames, Iowa and display the standard deviation of their housing prices: 
```{r}
n=30
index = sample(1:nrow(ames), n)
samp=ames[index,]
sd(samp$price)
```

How does this outcome compare to what you found in (a)?

(d) Run the following code to, for trials=1000 times, sample 30 new random houses from Ames, Iowa and record the standard deviation of their housing prices, and display a histogram of the standard deviations: 
```{r}
n=30
trials=1000
sdvec=rep(NA, trials)

for(i in 1:trials){
  index = sample(1:nrow(ames), n)
  samp=ames[index,]
  sdvec[i]=sd(samp$price)  
}

hist(sdvec, breaks=25)
```

What do we notice about the possible variation in sample standard deviations?

(e) Run the following code to re-display the above histogram, along with the actual standard deviation of housing prices in Ames, Iowa: 
```{r}
hist(sdvec, breaks=25)
abline(v = sd(ames$price), col="red", lwd=3, lty=2)
```

What do we notice about the possible variation in sample standard deviations?

(f) To see how much variation there could be in sampling distributions, run the following code to plot a normal curve for each sample standard deviation we found, each curve has mean 0, but standard deviation equal to one we found in our above simulation: 
```{r}
values = seq(-300000, 300000, length=100)

plot(values, dnorm(values, mean=0, sd=sdvec[1]), type="l", lty=2)

for (i in 2:trials){
  lines(values, dnorm(values, mean=0, sd=sdvec[i])) 
}
```
How much variation is there between curves, even with the same means?

(g) Run the following code to, for `trials=1000` times, sample now `n=500` new random houses from Ames, Iowa and record the standard deviation of their housing prices, and display a histogram of the standard deviations: 
```{r}
n=500
trials=1000
sdvec500=rep(NA, trials)

for(i in 1:trials){
  index = sample(1:nrow(ames), n)
  samp=ames[index,]
  sdvec500[i]=sd(samp$price)  
}
```

What do we notice about the possible variation in sample standard deviations compared to what we saw in (d)?

## Subsection 6.1.2 The \(t\)-distribution

### Definition 6.1.4. \(t\)-Random Variable.

Looking at Activity 6.1.2, we notice:

*	A normal curve where we use a sample standard deviation \(s\) in lieu of \(\sigma\) may be highly inaccurate, since there is a lot of variation of what \(s\) is in comparison to \(\sigma\).

*	This variation decreases as we increase \(n\) the size of the sample.

To account for this, we introduce what we call the standard \(t\)-distribution (mean 0, standard deviation 1). The technical formula for a \(t\)-distribution is uninteresting and tedious. The main thing we want to understand about the standard \(t\)-distribution with mean \(0\) and standard deviation 1, is that it has an additional parameter: degrees of freedom: \(d_f\).For a \(t\)-distribution, the number of degrees of freedom is \(d_f=n-1\) where \(n\) is the sample size.

We notice that when \(d_f\) is small, the \(t\)-distribution resembles a much widened normal distribution to account for the variability in the sample standard deviation. As \(d_f\) increases, the \(t\)-distribution grows to resemble a normal distribution. 

### Activity 6.1.3. Probabilities and the \(t\)-Distribution. 
Probabilities of \(t\) variables are computed in a manner similar to that of normal variables, via areas as in Activity 3.1.2.

We can also confirm this by running the following: 

(a) For a \(t\)-variable with 19 degrees of freedom, compute the probability \(t\) is greater than 1.1: \(P(t_{19}>1.1)\).
```{r}
pt(1.1, 19, lower.tail = FALSE)
```

(b) For a \(t\)-variable with 100 degrees of freedom, compute the probability \(t\) is between -2 and 1.5: \(P(-2\lt t_{100}\lt1.5)\).
```{r}
pt(-2, 100, lower.tail = FALSE) - pt(1.5, 100, lower.tail = FALSE)

pt(1.5, 100, lower.tail = TRUE) - pt(-2, 100, lower.tail = TRUE)
```

(c) For \(t\) variables with degrees of freedom 5, 20 and 200, compute \(P(-1\lt t\lt1)\).What do we notice as the degrees of freedom increase?
```{r}
df = c(5,20,200)
pt(-1, df, lower.tail = FALSE) - pt(1, df, lower.tail = FALSE)
```

(d) For \(t\) variables with degrees of freedom 5, 20 and 200, compute \(P(t>2)\).What do we notice as the degrees of freedom increase?
```{r}

```

### Activity 6.1.4. Probabilities and the \(t\)-Distribution: Inverses. 
Just as for normal variables (see: Activity 3.1.3), given an area or probability, we should be able to recover bounds for it.
Below, we can see that for a \(t\) variable with 13 degrees of freedom, if we wanted to find a value \(t_{13}^*\) so the probability that \(t\) is less than \(t_{13}^*\) is 70%, or \(P(t_{13}\lt t_{13}^*)=0.7\) ,we can see that the value is about \(t_{13}^*\approx 0.5375\).
We can also confirm this by running the following: 

(a) For a \(t\)-variable with 13 degrees of freedom, find \(t^*_{13}\) such that \(P(t_{13}>t^*_{13})=0.45\).
```{r}
qt(.45, 13,lower.tail = FALSE)
```

(b) For a \(t\)-variable with 45 degrees of freedom, find \(t_{45}^*\) such that \(P(t_{45}\lt t_{45}^*)=0.55\).
```{r}

```

(c) For \(t\) variables with degrees of freedom 5, 20 and 200, find find \(t^*\) such that \(P(-t^*\lt t\lt t^*)=0.85\).What do we notice as the degrees of freedom increase?
```{r}

```

### Activity 6.1.5. \(t\)-score for Sampling Distribution Variables. 
Just as for general normal variables, most sampling distributions do not have mean 0 and standard deviation 1. Just as with general normal variables, we each value of the sampling distribution has a corresponding \(t\)-score on our general \(t\)-distribution .

Consider a sampling distribution with mean \(\mu\) and standard deviation \(SE\).For each value \(X\) from this distribution, we can compute the \(t\)-score for \(X\) via:
\begin{equation*} t=\frac{X-\mu}{SE} \end{equation*} 
as in Definition 3.2.1.

(a) For a sampling distribution with mean \(\mu=45\) ,standard error \(SE=12\) find the \(t\)-score of \(X=60\).
```{r}
t = (60-45)/12
t
```

(b) For a sampling distribution with mean \(\mu=120\) ,standard error \(SE=26\) find the \(X\) values for the \(t\)-scores: -1.97, 1.97.
```{r}
X = -1.97*26+120
X
X = 1.97*FIXME + FIXME
X
```

### Activity 6.1.6. Putting it Together. 
Suppose you sampled a numerical variable 20 times, and obtained the following values:
 20, 36, 23, 38, 42, 29, 47, 23, 30, 37, 45, 44, 34, 19, 31, 30, 14, 48, 50, 18. 
 
(a) Compute the sample mean \(\bar{x}\), and the standard deviation \(s\) ,for the above sample.
```{r}
list = c(20, 36, 23, 38, 42, 29, 47, 23, 30, 37, 45, 44, 34, 19, 31, 30, 14, 48, 50, 18)
mean_list =mean(list)
sd_list = sd(list)
mean_list
sd_list
```

(b) What is \(n\) the size of the sample? Use Theorem 6.1.2 to compute the standard error for the sampling distribution: \(SE\).
```{r}
n_list = length(list)
SE_list = FIXME
```

(c) Use Definition 6.1.4 to compute the degrees of freedom of the sampling distribution.
```{r}
df = n-1
```

(d) If we assume the sampling distribution has mean \(\mu=30\) ,use this mean and \(SE\) to compute a \(t\)-score for \(\bar{x}\), \(t_{\bar{x}}\).
```{r}
t_x = (FIXME - 30)/SE_list
```

(e) Compute \(P(t>t_{\bar{x}})\).
```{r}
pt(t_x,df,lower.tail = FALSE)
```
